# core/datasets.py (ä¿®æ”¹å¾Œçš„è‰²å½©åŒ–dataloader - æ”¯æ´4å¹€åºåˆ—)

import os
import glob
import random
import numpy as np
import cv2
import torch
from PIL import Image
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms


class VideoColorizationDataset(Dataset):
    """
    è¦–é »ä¸Šè‰²æ•¸æ“šé›† - 4å¹€åºåˆ—ç‰ˆæœ¬
    é©é…MemFlowNetçš„è¼¸å…¥æ ¼å¼
    """
    def __init__(self, 
                 video_data_root_list,
                 image_size=[384, 512],
                 min_frames=4,  # â† æ”¹æˆ4
                 augment=True):
        """
        Args:
            video_data_root_list: è¦–é »æ•¸æ“šæ ¹ç›®éŒ„åˆ—è¡¨
            image_size: [H, W]
            min_frames: æœ€å°‘å¹€æ•¸ (å¿…é ˆâ‰¥4)
            augment: æ˜¯å¦æ•¸æ“šå¢å¼·
        """
        self.video_data_root_list = video_data_root_list if isinstance(video_data_root_list, list) else [video_data_root_list]
        self.image_size = image_size
        self.min_frames = max(min_frames, 4)  # ç¢ºä¿è‡³å°‘4å¹€
        self.augment = augment
        
        # ImageNetæ¨™æº–åŒ–
        self.normalize = transforms.Normalize(
            mean=[0.485, 0.456, 0.406],
            std=[0.229, 0.224, 0.225]
        )
        self.to_tensor = transforms.ToTensor()
        
        print(f"ğŸ“‚ Loading dataset from {len(self.video_data_root_list)} paths...")
        for i, path in enumerate(self.video_data_root_list):
            print(f"   Path {i+1}: {path}")
            
        self.frame_sequences = self._load_frame_sequences()  # â† æ”¹å
        print(f"ğŸ“Š Found {len(self.frame_sequences)} 4-frame sequences")
        print(f"ğŸ¬ Scenes: {len(set(s['scene_id'] for s in self.frame_sequences))}")

    def _load_frame_sequences(self):
        """è¼‰å…¥æ‰€æœ‰é€£çºŒ4å¹€åºåˆ—"""
        all_sequences = []
        
        for root_idx, video_data_root in enumerate(self.video_data_root_list):
            if not os.path.exists(video_data_root):
                print(f"âš ï¸ Path not found: {video_data_root}")
                continue
                
            seq_count = 0
            for item in os.listdir(video_data_root):
                item_path = os.path.join(video_data_root, item)
                if os.path.isdir(item_path):
                    # æ”¶é›†æ‰€æœ‰åœ–åƒ
                    frames = []
                    for ext in ['*.jpg', '*.jpeg', '*.png']:
                        frames.extend(glob.glob(os.path.join(item_path, ext)))
                    
                    frames = sorted(frames)
                    if len(frames) >= self.min_frames:
                        unique_scene_id = f"path{root_idx}_{item}"
                        
                        # â† é—œéµä¿®æ”¹: ç”Ÿæˆé€£çºŒ4å¹€åºåˆ—
                        for i in range(len(frames) - 3):  # -3 å› ç‚ºéœ€è¦4å¹€
                            all_sequences.append({
                                'scene_id': unique_scene_id,
                                'frame_paths': [
                                    frames[i],
                                    frames[i + 1],
                                    frames[i + 2],
                                    frames[i + 3]
                                ],
                                'start_idx': i,
                                'total_frames': len(frames)
                            })
                            seq_count += 1
                            
            print(f"   âœ… Path {root_idx+1}: {seq_count} sequences")

        random.shuffle(all_sequences)
        return all_sequences

    def _load_and_process_image(self, path):
        """
        è¼‰å…¥ä¸¦è™•ç†å–®å¼µåœ–åƒ
        
        Returns:
            rgb_gray: [3, H, W] - ImageNetæ¨™æº–åŒ–çš„ç°éšRGBï¼ˆçµ¦SwinV2ï¼‰
            lab: [3, H, W] - LABæ ¼å¼ (L:[0,100], ab:[-128,127])
        """
        try:
            # è¼‰å…¥ä¸¦èª¿æ•´å¤§å°
            image = Image.open(path).convert('RGB')
            image = image.resize((self.image_size[1], self.image_size[0]), Image.LANCZOS)
            
            # ===== 1. è™•ç† RGB ç°éšï¼ˆçµ¦ SwinV2ï¼‰ =====
            image_gray = image.convert('L')
            gray_tensor = self.to_tensor(image_gray)  # [1, H, W]
            rgb_gray = gray_tensor.repeat(3, 1, 1)    # [3, H, W]
            rgb_gray = self.normalize(rgb_gray)       # ImageNetæ¨™æº–åŒ–
            
            # ===== 2. è™•ç† LAB =====
            image_np = np.array(image, dtype=np.uint8)
            lab_np = cv2.cvtColor(image_np, cv2.COLOR_RGB2LAB).astype(np.float32)
            
            # LAB è½‰æ›åˆ°æ¨™æº–ç¯„åœ
            lab_np[:, :, 0] = lab_np[:, :, 0] * 100.0 / 255.0  # L: [0,100]
            lab_np[:, :, 1] = lab_np[:, :, 1] - 128.0          # a: [-128,127]
            lab_np[:, :, 2] = lab_np[:, :, 2] - 128.0          # b: [-128,127]
            
            lab = torch.from_numpy(lab_np).permute(2, 0, 1)  # [3, H, W]
            
            return rgb_gray, lab
            
        except Exception as e:
            print(f"âŒ Error loading {path}: {e}")
            # è¿”å›é»˜èªå€¼
            rgb_gray = torch.zeros(3, self.image_size[0], self.image_size[1])
            lab = torch.zeros(3, self.image_size[0], self.image_size[1])
            lab[0] = 50.0
            return rgb_gray, lab

    def _apply_augmentation(self, rgb_list, lab_list):
        """
        åŒæ­¥æ•¸æ“šå¢å¼· - æ‡‰ç”¨åˆ°æ‰€æœ‰4å¹€
        
        Args:
            rgb_list: list of [3, H, W]
            lab_list: list of [3, H, W]
        """
        if not self.augment:
            return rgb_list, lab_list
        
        # æ°´å¹³ç¿»è½‰ï¼ˆåŒæ­¥æ‰€æœ‰å¹€ï¼‰
        if random.random() > 0.5:
            rgb_list = [torch.flip(rgb, [-1]) for rgb in rgb_list]
            lab_list = [torch.flip(lab, [-1]) for lab in lab_list]
        
        # äº®åº¦èª¿æ•´ï¼ˆåªå°LABçš„Lé€šé“,æ‰€æœ‰å¹€åŒæ­¥ï¼‰
        if random.random() > 0.7:
            factor = random.uniform(0.8, 1.2)
            for lab in lab_list:
                lab[0] = torch.clamp(lab[0] * factor, 0, 100)
        
        # é£½å’Œåº¦èª¿æ•´ï¼ˆåªå°LABçš„abé€šé“,æ‰€æœ‰å¹€åŒæ­¥ï¼‰
        if random.random() > 0.7:
            factor = random.uniform(0.8, 1.2)
            for lab in lab_list:
                lab[1:3] = torch.clamp(lab[1:3] * factor, -128, 127)
        
        return rgb_list, lab_list

    def __len__(self):
        return len(self.frame_sequences)

    def __getitem__(self, idx):
        seq_info = self.frame_sequences[idx]
        
        # â† é—œéµä¿®æ”¹: è¼‰å…¥4å¹€
        rgb_list = []
        lab_list = []
        for frame_path in seq_info['frame_paths']:
            rgb, lab = self._load_and_process_image(frame_path)
            rgb_list.append(rgb)
            lab_list.append(lab)
        
        # æ•¸æ“šå¢å¼·ï¼ˆåŒæ­¥æ‰€æœ‰å¹€ï¼‰
        rgb_list, lab_list = self._apply_augmentation(rgb_list, lab_list)
        
        # â† é—œéµä¿®æ”¹: Stackæˆåºåˆ—æ ¼å¼
        rgb_seq = torch.stack(rgb_list, dim=0)  # [4, 3, H, W]
        lab_seq = torch.stack(lab_list, dim=0)  # [4, 3, H, W]
        
        # ===== æº–å‚™è¼¸å‡º (MemFlowNetæ ¼å¼) =====
        return {
            'images': lab_seq,  # [4, 3, H, W] - LABåºåˆ—
            'rgb_inputs': rgb_seq,  # [4, 3, H, W] - ç°éšRGBåºåˆ—
            'scene_id': seq_info['scene_id']
        }


def fetch_dataloader(args):
    """
    å‰µå»ºDataLoader
    
    Args:
        args.data_path: æ•¸æ“šè·¯å¾‘,é€—è™Ÿåˆ†éš”å¤šå€‹è·¯å¾‘
        args.batch_size: batchå¤§å°
        args.image_size: [H, W]
    """
    # è§£ææ•¸æ“šè·¯å¾‘
    data_paths = args.data_path.split(',')
    data_paths = [p.strip() for p in data_paths]
    
    # å‰µå»ºæ•¸æ“šé›†
    train_dataset = VideoColorizationDataset(
        video_data_root_list=data_paths,
        image_size=args.image_size,
        min_frames=4,
        augment=True
    )
    
    # å‰µå»ºDataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=8,
        pin_memory=True,
        drop_last=True
    )
    
    print(f'âœ… Training with {len(train_dataset)} 4-frame sequences')
    return train_loader


