# inference_colorization.py - MemFlowNetè¦–é »è‰²å½©åŒ–æ¨ç†è…³æœ¬ (Autoregressive)

from __future__ import print_function, division
import sys
sys.path.append('core')

import argparse
import os
import glob
import numpy as np
import torch
import torch.nn as nn
import cv2
from PIL import Image
from pathlib import Path
from tqdm import tqdm
import matplotlib
matplotlib.use('Agg')  # éäº¤äº’å¼å¾Œç«¯
import matplotlib.pyplot as plt
from matplotlib import cm

from core.Networks import build_network
from core.loss_new import warp_color_by_flow
from loguru import logger as loguru_logger


class ColorizationInferenceCore:
    """è‰²å½©åŒ–æ¨ç†æ ¸å¿ƒ - å¸¶memoryç®¡ç†"""
    
    def __init__(self, model, config):
        self.model = model
        self.config = config
        self.clear_memory()
        
    def clear_memory(self):
        """æ¸…ç©ºmemory (æ¯å€‹æ–°è¦–é »é–‹å§‹æ™‚èª¿ç”¨)"""
        self.curr_ti = -1
        self.values = None  # Memory buffer
        
    def step(self, images_norm, query, key, net, inp, coords0, coords1, fmaps):
        """
        è™•ç†ä¸€å¹€
        
        Args:
            images_norm: [1, 2, 3, H, W] - ç•¶å‰å¹€å’Œä¸‹ä¸€å¹€çš„LABæ­¸ä¸€åŒ–
            query, key, net, inp: context encodingçµæœ
            coords0, coords1: åˆå§‹åæ¨™
            fmaps: ç‰¹å¾µåœ–
            
        Returns:
            flow_final: [1, 2, H/8, W/8] - flowé æ¸¬
            current_value: memory value
        """
        self.curr_ti += 1
        B = images_norm.shape[0]
        
        # Memoryç®¡ç†
        if self.curr_ti == 0:
            # ç¬¬ä¸€å¹€: æ²’æœ‰æ­·å²
            ref_values = None
            ref_keys = key.unsqueeze(2)  # [B, C, 1, H, W]
        elif self.curr_ti < self.config.num_ref_frames:
            # å‰å¹¾å¹€: ä½¿ç”¨æ‰€æœ‰æ­·å²
            ref_values = self.values
            # ç´¯ç©æ‰€æœ‰æ­·å²keys + ç•¶å‰key
            all_keys = []
            for ti in range(self.curr_ti):
                all_keys.append(self.values[:, :, ti:ti+1])
            all_keys.append(key.unsqueeze(2))
            ref_keys = torch.cat(all_keys, dim=2)
        else:
            # æ­·å²éå¤š: éš¨æ©Ÿæ¡æ¨£
            indices = [torch.randperm(self.curr_ti)[:self.config.num_ref_frames - 1] for _ in range(B)]
            ref_values = torch.stack([
                self.values[bi, :, indices[bi]] for bi in range(B)
            ], 0)
            ref_keys = torch.stack([
                self.values[bi, :, indices[bi]] for bi in range(B)
            ], 0)
            ref_keys = torch.cat([ref_keys, key.unsqueeze(2)], dim=2)
        
        # Predict flow
        flow_predictions, current_value, confidence_map = self.model.predict_flow(
            net,
            inp,
            coords0,
            coords1,
            fmaps,
            query.unsqueeze(2),  # åŠ æ™‚é–“ç¶­åº¦ [B, C, 1, H, W]
            ref_keys,
            ref_values
        )

        # å–æœ€å¾Œä¸€æ¬¡è¿­ä»£çš„çµæœ
        flow_final = flow_predictions[-1]

        # ç´¯ç©memory value
        if self.values is None:
            self.values = current_value
        else:
            self.values = torch.cat([self.values, current_value], dim=2)

        return flow_final, current_value, confidence_map


def load_image_as_lab(image_path, target_size=(224, 224)):
    """
    è¼‰å…¥åœ–åƒä¸¦è½‰æ›ç‚ºLABæ ¼å¼
    
    Args:
        image_path: åœ–åƒè·¯å¾‘
        target_size: (H, W) ç›®æ¨™å°ºå¯¸
        
    Returns:
        lab: [3, H, W] tensor, LABæ ¼å¼
        original_size: (H, W) åŸå§‹å°ºå¯¸
    """
    # è¼‰å…¥åœ–åƒ
    image = Image.open(image_path).convert('RGB')
    original_size = image.size  # (W, H)
    
    # Resizeåˆ°ç›®æ¨™å°ºå¯¸
    image = image.resize((target_size[1], target_size[0]), Image.LANCZOS)
    image_np = np.array(image, dtype=np.uint8)
    
    # è½‰æ›åˆ°LAB
    image_lab = cv2.cvtColor(image_np, cv2.COLOR_RGB2LAB).astype(np.float32)
    
    # æ¨™æº–åŒ–ç¯„åœ
    image_lab[:, :, 0] = image_lab[:, :, 0] * 100.0 / 255.0  # L: [0,100]
    image_lab[:, :, 1] = image_lab[:, :, 1] - 128.0          # a: [-128,127]
    image_lab[:, :, 2] = image_lab[:, :, 2] - 128.0          # b: [-128,127]
    
    # è½‰ç‚ºtensor
    lab = torch.from_numpy(image_lab).permute(2, 0, 1)  # [3, H, W]
    
    return lab, original_size


def lab_to_rgb(lab_tensor):
    """
    LAB tensorè½‰RGB numpy
    
    Args:
        lab_tensor: [3, H, W] tensor, LABæ ¼å¼
        
    Returns:
        rgb_np: [H, W, 3] numpy array, RGBæ ¼å¼
    """
    lab_np = lab_tensor.permute(1, 2, 0).cpu().numpy()
    
    # è½‰å›OpenCV LABæ ¼å¼
    lab_cv = lab_np.copy()
    lab_cv[:, :, 0] = lab_np[:, :, 0] * 255.0 / 100.0  # L
    lab_cv[:, :, 1] = lab_np[:, :, 1] + 128.0          # a
    lab_cv[:, :, 2] = lab_np[:, :, 2] + 128.0          # b
    
    lab_cv = np.clip(lab_cv, 0, 255).astype(np.uint8)
    
    # è½‰RGB
    bgr_np = cv2.cvtColor(lab_cv, cv2.COLOR_LAB2BGR)
    rgb_np = cv2.cvtColor(bgr_np, cv2.COLOR_BGR2RGB)
    
    return rgb_np


def visualize_confidence_map(confidence_np, cmap='viridis'):
    """
    å°‡ confidence map è½‰æ›ç‚ºè¦–è¦ºåŒ–çš„å½©è‰²åœ–åƒ

    Args:
        confidence_np: [H, W] numpy array, å€¼åŸŸ [0, 1]
        cmap: matplotlib colormap åç¨± ('viridis', 'jet', 'hot', 'plasma' ç­‰)

    Returns:
        vis_image: [H, W, 3] numpy array, RGBæ ¼å¼, å€¼åŸŸ [0, 255]
    """
    # ç¢ºä¿å€¼åŸŸåœ¨ [0, 1]
    confidence_np = np.clip(confidence_np, 0, 1)

    # ä½¿ç”¨ matplotlib colormap å°‡å€¼æ˜ å°„åˆ°é¡è‰²
    colormap = cm.get_cmap(cmap)
    colored = colormap(confidence_np)  # [H, W, 4] (RGBA)

    # è½‰æ›ç‚º RGB (å»æ‰ alpha é€šé“) ä¸¦ç¸®æ”¾åˆ° [0, 255]
    rgb = (colored[:, :, :3] * 255).astype(np.uint8)

    return rgb


@torch.no_grad()
def colorize_video(model, video_frames, processor, target_size=(224, 224)):
    """
    å°ä¸€å€‹è¦–é »åºåˆ—é€²è¡Œè‰²å½©åŒ– (Autoregressiveæ¨ç†)

    Args:
        model: MemFlowNetæ¨¡å‹
        video_frames: list of frame paths
        processor: ColorizationInferenceCoreå¯¦ä¾‹
        target_size: (H, W)

    Returns:
        colorized_frames: list of RGB numpy arrays
        confidence_maps: list of confidence numpy arrays
    """
    processor.clear_memory()  # æ¸…ç©ºmemory

    colorized_frames = []
    confidence_maps = []

    # ===== ç¬¬ä¸€å¹€: ä½¿ç”¨GT (reference frame) =====
    first_lab, _ = load_image_as_lab(video_frames[0], target_size)
    first_rgb = lab_to_rgb(first_lab)
    colorized_frames.append(first_rgb)
    # ç¬¬ä¸€å¹€æ²’æœ‰ confidenceï¼ˆå› ç‚ºæ˜¯ GTï¼‰
    confidence_maps.append(None)
    
    # ä¿å­˜ç¬¬ä¸€å¹€çš„ABä½œç‚ºèµ·å§‹
    last_predicted_ab_norm = first_lab[1:3] / 127.0  # æ­¸ä¸€åŒ–åˆ°[-1, 1]
    
    # ===== å¾ç¬¬äºŒå¹€é–‹å§‹autoregressiveæ¨ç† =====
    for i in tqdm(range(len(video_frames) - 1), desc="  Colorizing", leave=False):
        # è¼‰å…¥ç•¶å‰å¹€å’Œä¸‹ä¸€å¹€çš„Lé€šé“
        frame_t_lab, _ = load_image_as_lab(video_frames[i], target_size)
        frame_t1_lab, _ = load_image_as_lab(video_frames[i+1], target_size)
        
        # æº–å‚™è¼¸å…¥: Lé€šé“(GT) + ABé€šé“(é æ¸¬)
        frame_t_norm = torch.zeros_like(frame_t_lab)
        frame_t1_norm = torch.zeros_like(frame_t1_lab)
        
        # Lé€šé“: ä½¿ç”¨GT
        frame_t_norm[0] = (frame_t_lab[0] / 50.0) - 1.0
        frame_t1_norm[0] = (frame_t1_lab[0] / 50.0) - 1.0
        
        # ABé€šé“: ä½¿ç”¨ä¸Šä¸€å¹€çš„é æ¸¬çµæœ (autoregressive!)
        frame_t_norm[1:3] = last_predicted_ab_norm
        frame_t1_norm[1:3] = last_predicted_ab_norm  # åˆå§‹çŒœæ¸¬,æœƒè¢«æ›´æ–°
        
        # Stackæˆ[1, 2, 3, H, W]
        images_norm = torch.stack([frame_t_norm, frame_t1_norm], dim=0).unsqueeze(0).cuda()
        
        H, W = target_size
        
        # Forward pass
        with torch.cuda.amp.autocast(enabled=True, dtype=torch.bfloat16):
            # Encode context (åªç”¨ç¬¬ä¸€å¹€)
            query, key, net, inp = model.encode_context(images_norm[:, 0, ...])
            
            # Encode features
            coords0, coords1, fmaps = model.encode_features(images_norm)
            
            # Predict flow
            flow_final, current_value, confidence_map = processor.step(
                images_norm, query, key, net, inp, coords0, coords1, fmaps
            )
        
        # Upsample flow to target resolution (correct method)
        flow_h, flow_w = flow_final.shape[2:]

        if flow_h != H or flow_w != W:
            # This should NOT happen if MemFlow is working correctly (convex upsampling)
            print(f"âš ï¸  Warning: flow resolution mismatch! Expected [{H}, {W}], got [{flow_h}, {flow_w}]")

            # Step 1: Resize spatially (without scaling values)
            flow_up = nn.functional.interpolate(
                flow_final,
                size=(H, W),
                mode='bilinear',
                align_corners=True
            )

            # Step 2: Scale flow values by resolution ratio (separately for x and y)
            flow_up[:, 0, :, :] *= (W / flow_w)  # x direction
            flow_up[:, 1, :, :] *= (H / flow_h)  # y direction
        else:
            # Flow is already at full resolution
            flow_up = flow_final
        
        # Color warping: å¾ä¸Šä¸€å¹€é æ¸¬warpåˆ°ç•¶å‰å¹€
        source_ab = last_predicted_ab_norm.unsqueeze(0).cuda()  # [1, 2, H, W]
        warped_ab = warp_color_by_flow(source_ab, flow_up)
        
        # æ›´æ–°last_predicted_ab_norm (ç”¨æ–¼ä¸‹ä¸€æ¬¡è¿­ä»£)
        last_predicted_ab_norm = warped_ab.squeeze(0).cpu()
        
        # çµ„åˆL + warped AB
        colorized_lab = torch.cat([
            frame_t1_lab[0:1],  # ä½¿ç”¨ç›®æ¨™å¹€çš„Lé€šé“ (GT)
            last_predicted_ab_norm * 127.0  # é æ¸¬çš„ABé€šé“
        ], dim=0)
        
        # è½‰RGB
        colorized_rgb = lab_to_rgb(colorized_lab)
        colorized_frames.append(colorized_rgb)

        # ä¿å­˜ confidence map (è½‰ç‚º numpy array)
        confidence_np = confidence_map.squeeze().cpu().numpy()  # [H, W]
        confidence_maps.append(confidence_np)

    return colorized_frames, confidence_maps


def process_video_directory(input_dir, output_dir, model, config, target_size=(224, 224)):
    """
    è™•ç†ä¸€å€‹è¦–é »æ–‡ä»¶å¤¾
    
    Args:
        input_dir: è¼¸å…¥è¦–é »æ–‡ä»¶å¤¾è·¯å¾‘
        output_dir: è¼¸å‡ºæ–‡ä»¶å¤¾è·¯å¾‘
        model: MemFlowNetæ¨¡å‹
        config: é…ç½®
        target_size: (H, W)
    """
    # ç²å–æ‰€æœ‰åœ–åƒ
    image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']
    frames = []
    for ext in image_extensions:
        frames.extend(glob.glob(os.path.join(input_dir, ext)))
        frames.extend(glob.glob(os.path.join(input_dir, ext.upper())))
    
    frames = sorted(frames)
    
    if len(frames) < 2:
        print(f"  âš ï¸ Skipping {input_dir}: need at least 2 frames")
        return
    
    print(f"  ğŸ“¹ Found {len(frames)} frames")
    
    # å‰µå»ºinference processor
    processor = ColorizationInferenceCore(model, config)
    
    # è‰²å½©åŒ–
    colorized_frames, confidence_maps = colorize_video(model, frames, processor, target_size)

    # ä¿å­˜çµæœ
    os.makedirs(output_dir, exist_ok=True)
    confidence_dir = os.path.join(output_dir, 'confidence')
    os.makedirs(confidence_dir, exist_ok=True)

    for i, (frame_path, colorized, confidence) in enumerate(zip(frames, colorized_frames, confidence_maps)):
        # ä¿æŒåŸå§‹æ–‡ä»¶å
        frame_name = os.path.basename(frame_path)
        output_path = os.path.join(output_dir, frame_name)

        # ä¿å­˜å½©è‰²åœ–åƒ
        Image.fromarray(colorized).save(output_path)

        # ä¿å­˜ confidence map è¦–è¦ºåŒ–åœ–åƒï¼ˆå¦‚æœä¸æ˜¯ Noneï¼‰
        if confidence is not None:
            # å°‡ confidence è½‰æ›ç‚ºè¦–è¦ºåŒ–åœ–åƒ
            conf_vis = visualize_confidence_map(confidence, cmap='viridis')

            # ä¿å­˜ç‚º PNG åœ–ç‰‡
            conf_name = os.path.splitext(frame_name)[0] + '_confidence.png'
            conf_path = os.path.join(confidence_dir, conf_name)
            Image.fromarray(conf_vis).save(conf_path)

    print(f"  âœ… Saved {len(colorized_frames)} frames to {output_dir}")
    print(f"  âœ… Saved {len([c for c in confidence_maps if c is not None])} confidence maps to {confidence_dir}")


def main():
    parser = argparse.ArgumentParser(description='MemFlowNetè¦–é »è‰²å½©åŒ–æ¨ç† (Autoregressive)')
    
    # åŸºæœ¬åƒæ•¸
    parser.add_argument('--input_dir', required=True, help='è¼¸å…¥æ•¸æ“šæ ¹ç›®éŒ„')
    parser.add_argument('--output_dir', required=True, help='è¼¸å‡ºæ•¸æ“šæ ¹ç›®éŒ„')
    parser.add_argument('--checkpoint', required=True, help='è¨“ç·´å¥½çš„checkpointè·¯å¾‘')
    parser.add_argument('--stage', default='colorization', help='stage name')
    
    # æ¨¡å‹åƒæ•¸
    parser.add_argument('--image_size', type=int, default=224, help='è™•ç†å°ºå¯¸')
    parser.add_argument('--GPU_ids', type=str, default='0', help='GPU ID')
    
    args = parser.parse_args()
    
    # è¨­ç½®GPU
    os.environ['CUDA_VISIBLE_DEVICES'] = args.GPU_ids
    
    # è¼‰å…¥é…ç½®
    print("="*60)
    print("ğŸ”§ Loading configuration...")
    print("="*60)
    
    from configs.colorization_memflownet import get_cfg
    cfg = get_cfg()
    cfg.restore_ckpt = args.checkpoint
    
    # è¼‰å…¥æ¨¡å‹
    print("\n" + "="*60)
    print("ğŸ”§ Loading model...")
    print("="*60)
    
    model = build_network(cfg).cuda()
    model = nn.DataParallel(model)
    
    # è¼‰å…¥checkpoint
    print(f"ğŸ“¥ Loading checkpoint: {args.checkpoint}")
    ckpt = torch.load(args.checkpoint, map_location='cpu')
    ckpt_model = ckpt['model'] if 'model' in ckpt else ckpt
    
    if 'module' in list(ckpt_model.keys())[0]:
        model.load_state_dict(ckpt_model, strict=False)
    else:
        model.module.load_state_dict(ckpt_model, strict=False)
    
    model.eval()
    print("âœ… Model loaded\n")
    
    # æƒæè¼¸å…¥ç›®éŒ„
    print("="*60)
    print(f"ğŸ“‚ Scanning input directory: {args.input_dir}")
    print("="*60)
    
    video_dirs = []
    for item in os.listdir(args.input_dir):
        item_path = os.path.join(args.input_dir, item)
        if os.path.isdir(item_path):
            video_dirs.append((item, item_path))
    
    video_dirs = sorted(video_dirs)
    print(f"ğŸ“Š Found {len(video_dirs)} video directories\n")
    
    # è™•ç†æ¯å€‹è¦–é »
    target_size = (args.image_size, args.image_size)
    
    for video_name, video_path in video_dirs:
        print(f"ğŸ¬ Processing: {video_name}")
        
        output_video_dir = os.path.join(args.output_dir, video_name)
        
        try:
            process_video_directory(
                video_path, 
                output_video_dir, 
                model.module, 
                cfg, 
                target_size
            )
        except Exception as e:
            print(f"  âŒ Error processing {video_name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    print("\n" + "="*60)
    print("âœ… All videos processed!")
    print(f"ğŸ“ Results saved to: {args.output_dir}")
    print("="*60)


if __name__ == '__main__':
    main()



